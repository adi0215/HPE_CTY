{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiuU4GSY5GprIASyQBqxtz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adi0215/HPE_CTY/blob/main/Callback_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OACWIj-hE4f8",
        "outputId": "bc068ab0-8b36-4b65-e03d-fca92dff927c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.9/dist-packages (1.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.9/dist-packages (from mxnet) (1.22.4)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.9/dist-packages (from mxnet) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install mxnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mxnet as mx\n",
        "from mxnet import gluon, autograd, nd\n",
        "from mxnet.gluon import nn"
      ],
      "metadata": {
        "id": "lDIjACHVIT9I"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple neural network\n",
        "net = nn.HybridSequential()\n",
        "with net.name_scope():\n",
        "    net.add(nn.Dense(128, activation='relu'))\n",
        "    net.add(nn.Dense(64, activation='relu'))\n",
        "    net.add(nn.Dense(10))\n",
        "\n",
        "ctx=mx.cpu()\n",
        "net.initialize(ctx=ctx)\n",
        "net.hybridize()"
      ],
      "metadata": {
        "id": "C3kSW765Idm8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an optimizer\n",
        "optimizer = gluon.Trainer(net.collect_params(),'adam')\n",
        "\n",
        "# Define a training dataset\n",
        "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=lambda data, label: (data.astype('float32')/255, label)), batch_size=32, shuffle=True)\n",
        "\n",
        "# Define a loss function\n",
        "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Go0k5qH1IkoW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "batch_size = 64\n",
        "train_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=True, transform=lambda data, label: (data.astype('float32')/255, label)), batch_size, shuffle=True)#splitting data for training\n",
        "val_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=False, transform=lambda data, label: (data.astype('float32')/255, label)), batch_size, shuffle=False)#splitting data for validation\n",
        "\n",
        "epoch_num=3"
      ],
      "metadata": {
        "id": "Yv9msPGwIp-O"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mxnet.gluon.contrib.estimator import estimator\n",
        "from mxnet.gluon.contrib.estimator.event_handler import TrainBegin, TrainEnd, EpochEnd\n",
        "\n",
        "# Define the estimator\n",
        "estimator = estimator.Estimator(net=net,\n",
        "                          loss=loss_fn,\n",
        "                          trainer=optimizer,\n",
        "                          context=ctx,\n",
        "                         )"
      ],
      "metadata": {
        "id": "GACTuGxQIt4y"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom event handler with TrainBegin, TrainEnd, EpochEnd callbacks\n",
        "class LossRecordHandler(TrainBegin, TrainEnd,EpochEnd):\n",
        "    def __init__(self):\n",
        "        super(LossRecordHandler, self).__init__()\n",
        "        self.loss_history = {}\n",
        "\n",
        "    def train_begin(self, estimator, *args, **kwargs):\n",
        "        print(\"Training starting \")\n",
        "        \n",
        "    def train_end(self, estimator, *args, **kwargs):\n",
        "        # Print all the losses at the end of training\n",
        "        print(\"Training ended\")\n",
        "        \n",
        "    def epoch_end(self, estimator, *args, **kwargs):\n",
        "        for metric in estimator.train_metrics:\n",
        "            # look for train Loss in training metrics\n",
        "            # we wrapped loss value as a metric to record it\n",
        "            if isinstance(metric, mx.metric.Loss):\n",
        "                loss_name, loss_val = metric.get()\n",
        "                # append loss value for this epoch\n",
        "                self.loss_history.setdefault(loss_name, []).append(loss_val)"
      ],
      "metadata": {
        "id": "fQrmiJKC-12a"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate Loss record handler \n",
        "loss_record_handler = LossRecordHandler()\n",
        "\n",
        "import warnings\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "# Train the model with the callback\n",
        "estimator.fit(train_data, epochs=epoch_num,event_handlers=[loss_record_handler])   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ams3C7NaL3qY",
        "outputId": "0679aaaf-1975-403c-c086-1cd00afb7781"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training starting \n",
            "Training begin: using optimizer Adam with current learning rate 0.0010 \n",
            "Train for 3 epochs.\n",
            "[Epoch 0] Begin, current learning rate: 0.0010\n",
            "[Epoch 0] Finished in 13.902s, training accuracy: 0.9933, training softmaxcrossentropyloss: 0.0226\n",
            "[Epoch 1] Begin, current learning rate: 0.0010\n",
            "[Epoch 1] Finished in 14.052s, training accuracy: 0.9937, training softmaxcrossentropyloss: 0.0188\n",
            "[Epoch 2] Begin, current learning rate: 0.0010\n",
            "[Epoch 2] Finished in 14.244s, training accuracy: 0.9949, training softmaxcrossentropyloss: 0.0160\n",
            "Training ended\n",
            "Train finished using total 42s with 3 epochs. training accuracy: 0.9949, training softmaxcrossentropyloss: 0.0160\n"
          ]
        }
      ]
    }
  ]
}